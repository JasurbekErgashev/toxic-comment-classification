{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8218148,"sourceType":"datasetVersion","datasetId":4871382}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-02T04:33:13.753249Z","iopub.execute_input":"2024-05-02T04:33:13.753942Z","iopub.status.idle":"2024-05-02T04:33:14.155395Z","shell.execute_reply.started":"2024-05-02T04:33:13.753910Z","shell.execute_reply":"2024-05-02T04:33:14.154477Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Reading dataset using pandas\ntrain_data = pd.read_csv('/kaggle/input/toxic-dataset/train_data.csv')\ntrain_data_external = pd.read_csv('/kaggle/input/toxic-dataset/train_data_external.csv')\ntest_data = pd.read_csv('/kaggle/input/toxic-dataset/test_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:22.955799Z","iopub.execute_input":"2024-05-02T04:33:22.956565Z","iopub.status.idle":"2024-05-02T04:33:23.982332Z","shell.execute_reply.started":"2024-05-02T04:33:22.956535Z","shell.execute_reply":"2024-05-02T04:33:23.981489Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Train dataset preparation\n* Adding new external toxic and non-toxic comments\n* Balanced dataset","metadata":{}},{"cell_type":"code","source":"# Renaming dataset columns\ntrain_data.rename(columns={'Label': 'label', 'Text': 'comment_text'}, inplace=True)\ntest_data.rename(columns={'ID': 'id', 'Text': 'comment_text'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:25.936242Z","iopub.execute_input":"2024-05-02T04:33:25.936917Z","iopub.status.idle":"2024-05-02T04:33:25.950797Z","shell.execute_reply.started":"2024-05-02T04:33:25.936890Z","shell.execute_reply":"2024-05-02T04:33:25.949828Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Distribution of train data labels\ndistribution = train_data['label'].value_counts()\nprint(distribution)\n\n# Dataset has only 837 toxic comments while having 6654 non-toxic comments\n# Which is a huge data imbalance","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:28.163782Z","iopub.execute_input":"2024-05-02T04:33:28.164495Z","iopub.status.idle":"2024-05-02T04:33:28.186568Z","shell.execute_reply.started":"2024-05-02T04:33:28.164465Z","shell.execute_reply":"2024-05-02T04:33:28.185472Z"},"trusted":true},"outputs":[{"name":"stdout","text":"label\n1    6654\n0     837\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Get more than 15 000 toxic comments from external dataset\ntoxic_comments = train_data_external[train_data_external['toxic'] == 1]\ncopied_toxic_comments = toxic_comments[['toxic', 'comment_text']].copy()\ncopied_toxic_comments.rename(columns={'toxic': 'label'}, inplace=True)\ncopied_toxic_comments['label'] = copied_toxic_comments['label'].replace({1: 0})\ncopied_toxic_comments.reset_index(drop=True, inplace=True)\nprint(copied_toxic_comments)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:30.444474Z","iopub.execute_input":"2024-05-02T04:33:30.444813Z","iopub.status.idle":"2024-05-02T04:33:30.471754Z","shell.execute_reply.started":"2024-05-02T04:33:30.444789Z","shell.execute_reply":"2024-05-02T04:33:30.470792Z"},"trusted":true},"outputs":[{"name":"stdout","text":"       label                                       comment_text\n0          0       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n1          0  Hey... what is it..\\n@ | talk .\\nWhat is it......\n2          0  Bye! \\n\\nDon't look, come or think of comming ...\n3          0  You are gay or antisemmitian? \\n\\nArchangel WH...\n4          0           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\n...      ...                                                ...\n15289      0  \"\\n\\n our previous conversation \\n\\nyou fuckin...\n15290      0                  YOU ARE A MISCHIEVIOUS PUBIC HAIR\n15291      0  Your absurd edits \\n\\nYour absurd edits on gre...\n15292      0  \"\\n\\nHey listen don't you ever!!!! Delete my e...\n15293      0  and i'm going to keep posting the stuff u dele...\n\n[15294 rows x 2 columns]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Merging datasets\nmerged_train_data = pd.concat([train_data, copied_toxic_comments], ignore_index=True)\nprint(merged_train_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:32.536807Z","iopub.execute_input":"2024-05-02T04:33:32.537611Z","iopub.status.idle":"2024-05-02T04:33:32.547156Z","shell.execute_reply.started":"2024-05-02T04:33:32.537583Z","shell.execute_reply":"2024-05-02T04:33:32.546070Z"},"trusted":true},"outputs":[{"name":"stdout","text":"       label                                       comment_text\n0          1                                              why .\n1          1              I still love you so much just priva .\n2          1                         I wish you every success .\n3          0  She may or may not be a Jew but she 's certain...\n4          1  I 'm just pointing out our version of mercy an...\n...      ...                                                ...\n22780      0  \"\\n\\n our previous conversation \\n\\nyou fuckin...\n22781      0                  YOU ARE A MISCHIEVIOUS PUBIC HAIR\n22782      0  Your absurd edits \\n\\nYour absurd edits on gre...\n22783      0  \"\\n\\nHey listen don't you ever!!!! Delete my e...\n22784      0  and i'm going to keep posting the stuff u dele...\n\n[22785 rows x 2 columns]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Distribution of merged dataset\ndistribution_merged = merged_train_data['label'].value_counts()\nprint(distribution_merged)\n\n# Dataset has more than 16 000 toxic comments while having ~6500 non-toxic comments\n# We can add another 10 000 non-toxic comments to make it balanced","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:34.500928Z","iopub.execute_input":"2024-05-02T04:33:34.501341Z","iopub.status.idle":"2024-05-02T04:33:34.508482Z","shell.execute_reply.started":"2024-05-02T04:33:34.501311Z","shell.execute_reply":"2024-05-02T04:33:34.507346Z"},"trusted":true},"outputs":[{"name":"stdout","text":"label\n0    16131\n1     6654\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Creating new column named as \"non_toxic\" : if comment doesn't belong in any of the class then \"non_toxic\" will be 1 else 0\ntrain_data_external['non_toxic'] = train_data_external.iloc[:,2:8].apply(lambda x: 1 if (sum(x)==0) else 0, axis=1)\nprint(train_data_external)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:36.204529Z","iopub.execute_input":"2024-05-02T04:33:36.205274Z","iopub.status.idle":"2024-05-02T04:33:37.708439Z","shell.execute_reply.started":"2024-05-02T04:33:36.205237Z","shell.execute_reply":"2024-05-02T04:33:37.707285Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n...                  ...                                                ...   \n159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  non_toxic  \n0           0             0        0       0       0              0          1  \n1           0             0        0       0       0              0          1  \n2           0             0        0       0       0              0          1  \n3           0             0        0       0       0              0          1  \n4           0             0        0       0       0              0          1  \n...       ...           ...      ...     ...     ...            ...        ...  \n159566      0             0        0       0       0              0          1  \n159567      0             0        0       0       0              0          1  \n159568      0             0        0       0       0              0          1  \n159569      0             0        0       0       0              0          1  \n159570      0             0        0       0       0              0          1  \n\n[159571 rows x 9 columns]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Get ~10 000 non-toxic comments from external dataset\nnon_toxic_comments = train_data_external[train_data_external['non_toxic'] == 1]\ncopied_non_toxic_comments = non_toxic_comments[['non_toxic', 'comment_text']].copy()\ncopied_non_toxic_comments.rename(columns={'non_toxic': 'label'}, inplace=True)\ncopied_non_toxic_comments = copied_non_toxic_comments.sample(n=10000, random_state=42)\ncopied_non_toxic_comments.reset_index(drop=True, inplace=True)\nprint(copied_non_toxic_comments)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:39.976691Z","iopub.execute_input":"2024-05-02T04:33:39.977070Z","iopub.status.idle":"2024-05-02T04:33:40.013275Z","shell.execute_reply.started":"2024-05-02T04:33:39.977040Z","shell.execute_reply":"2024-05-02T04:33:40.012069Z"},"trusted":true},"outputs":[{"name":"stdout","text":"      label                                       comment_text\n0         1  \"\\n\\nOh, don't worry about me, Sandstein. I'm ...\n1         1               Are you trying to dispute that fact?\n2         1  SWOT analysis \\n\\nThis source – Align Technolo...\n3         1  cover \\n\\nso, do we want a current or older co...\n4         1  P.S. It's probably worth setting up a template...\n...     ...                                                ...\n9995      1  Because you read it in the Splinter Cell wiki?...\n9996      1  Do you have a source other than your opinion f...\n9997      1            REDIRECT Talk:River Rescue (video game)\n9998      1  I do not blame you. I was basically gang raped...\n9999      1  \"\\nNancy Pelosi is a high ranking official of ...\n\n[10000 rows x 2 columns]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Merging datasets\nfinal_train_data = pd.concat([merged_train_data, copied_non_toxic_comments], ignore_index=True)\nprint(final_train_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:44.314586Z","iopub.execute_input":"2024-05-02T04:33:44.314920Z","iopub.status.idle":"2024-05-02T04:33:44.324467Z","shell.execute_reply.started":"2024-05-02T04:33:44.314894Z","shell.execute_reply":"2024-05-02T04:33:44.323372Z"},"trusted":true},"outputs":[{"name":"stdout","text":"       label                                       comment_text\n0          1                                              why .\n1          1              I still love you so much just priva .\n2          1                         I wish you every success .\n3          0  She may or may not be a Jew but she 's certain...\n4          1  I 'm just pointing out our version of mercy an...\n...      ...                                                ...\n32780      1  Because you read it in the Splinter Cell wiki?...\n32781      1  Do you have a source other than your opinion f...\n32782      1            REDIRECT Talk:River Rescue (video game)\n32783      1  I do not blame you. I was basically gang raped...\n32784      1  \"\\nNancy Pelosi is a high ranking official of ...\n\n[32785 rows x 2 columns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Distribution of final train dataset\ndistribution_final = final_train_data['label'].value_counts()\nprint(distribution_final)\n\n# Dataset has balanced toxic and non-toxic comments, shuffled","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:46.039924Z","iopub.execute_input":"2024-05-02T04:33:46.040766Z","iopub.status.idle":"2024-05-02T04:33:46.047059Z","shell.execute_reply.started":"2024-05-02T04:33:46.040734Z","shell.execute_reply":"2024-05-02T04:33:46.045970Z"},"trusted":true},"outputs":[{"name":"stdout","text":"label\n1    16654\n0    16131\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Text Cleaning\n* Lowercase\n* Expanding contradictions\n* Removing URLs\n* Removing non-ASCII characters\n* Removing special characters (symbols & emojis)\n* Removing HTML\n* Removing escape characters\n* Removing punctuations and spaces which appeared more than once\n* Removing stop words","metadata":{}},{"cell_type":"code","source":"# Copied for further text cleaning\ncleaned_train_data = final_train_data.copy()\ncleaned_test_data = test_data.copy()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:48.377182Z","iopub.execute_input":"2024-05-02T04:33:48.377933Z","iopub.status.idle":"2024-05-02T04:33:48.382860Z","shell.execute_reply.started":"2024-05-02T04:33:48.377902Z","shell.execute_reply":"2024-05-02T04:33:48.382040Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"cleaned_train_data.to_csv('cleaned_train_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:35:06.648452Z","iopub.execute_input":"2024-05-02T04:35:06.648831Z","iopub.status.idle":"2024-05-02T04:35:07.093928Z","shell.execute_reply.started":"2024-05-02T04:35:06.648801Z","shell.execute_reply":"2024-05-02T04:35:07.093142Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Intalling the contractions package\n!pip install contractions","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:30:27.189623Z","iopub.execute_input":"2024-05-02T01:30:27.190451Z","iopub.status.idle":"2024-05-02T01:30:42.407688Z","shell.execute_reply.started":"2024-05-02T01:30:27.190418Z","shell.execute_reply":"2024-05-02T01:30:42.406557Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import contractions\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\n\n# Function which performs text cleaning\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Expand contractions\n    text = contractions.fix(text)\n    # Remove URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    # Remove non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7f]', '', text)\n    # Remove special characters, including symbols, emojis, and other graphic characters\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    # Remove HTML\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    text = re.sub(html, \"\", text)\n    # Remove escape characters\n    text = re.sub(r'[\\n\\t\\r\\a]', ' ', text)\n    # Replacing \"\" with \"\n    text = re.sub(r\"\\\"\\\"\", \"\\\"\", text)\n    # Removing quotation from start and the end of the string\n    text = re.sub(r\"^\\\"\", \"\", text)\n    text = re.sub(r\"\\\"$\", \"\", text)\n    # Removing Punctuation / Special characters (;:'\".?@!%&*+) which appears more than twice in the text\n    text = re.sub(r\"[^a-zA-Z0-9\\s][^a-zA-Z0-9\\s]+\", \" \", text)\n    # Removing Special characters \n    text = re.sub(r\"[^a-zA-Z0-9\\s\\\"\\',:;?!.()]\", \" \", text)\n    # Removing extra spaces in text\n    text = re.sub(r\"\\s\\s+\", \" \", text)\n    # Remove stop words\n    text = ' '.join(word for word in text.split() if word not in stop)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:33:47.356904Z","iopub.execute_input":"2024-05-02T01:33:47.357871Z","iopub.status.idle":"2024-05-02T01:33:48.603053Z","shell.execute_reply.started":"2024-05-02T01:33:47.357825Z","shell.execute_reply":"2024-05-02T01:33:48.602097Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Applying the clean_text function to the 'comment_text' column of the datasets\ncleaned_train_data['comment_text'] = cleaned_train_data['comment_text'].apply(clean_text)\ncleaned_test_data['comment_text'] = cleaned_test_data['comment_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:33:52.816338Z","iopub.execute_input":"2024-05-02T01:33:52.816699Z","iopub.status.idle":"2024-05-02T01:33:57.030890Z","shell.execute_reply.started":"2024-05-02T01:33:52.816672Z","shell.execute_reply":"2024-05-02T01:33:57.030057Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\ncleaned_train_data.to_csv('cleaned_train_data.csv', index=False)\ncleaned_test_data.to_csv('cleaned_test_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:34:12.222032Z","iopub.execute_input":"2024-05-02T01:34:12.222728Z","iopub.status.idle":"2024-05-02T01:34:12.519200Z","shell.execute_reply.started":"2024-05-02T01:34:12.222695Z","shell.execute_reply":"2024-05-02T01:34:12.518440Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:06.401458Z","iopub.execute_input":"2024-05-02T01:40:06.402335Z","iopub.status.idle":"2024-05-02T01:40:06.407008Z","shell.execute_reply.started":"2024-05-02T01:40:06.402297Z","shell.execute_reply":"2024-05-02T01:40:06.406059Z"},"trusted":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nEPOCHS = 3\nLEARNING_RATE = 2e-05","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:08.794427Z","iopub.execute_input":"2024-05-02T01:40:08.795328Z","iopub.status.idle":"2024-05-02T01:40:08.799828Z","shell.execute_reply.started":"2024-05-02T01:40:08.795290Z","shell.execute_reply":"2024-05-02T01:40:08.798711Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Separating data into training and validation sets\ntrain_df, val_df = train_test_split(cleaned_train_data, test_size=0.15, random_state=42)\n\nprint('Training data shape:', train_df.shape)\nprint('Validation data shape:', val_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:10.562127Z","iopub.execute_input":"2024-05-02T01:40:10.562885Z","iopub.status.idle":"2024-05-02T01:40:10.574677Z","shell.execute_reply.started":"2024-05-02T01:40:10.562848Z","shell.execute_reply":"2024-05-02T01:40:10.573809Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training data shape: (27867, 2)\nValidation data shape: (4918, 2)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"class DistilBERT_Model(nn.Module):\n    def __init__(self, num_labels):\n        super(DistilBERT_Model, self).__init__()\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.distilbert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.linear(pooled_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:12.743996Z","iopub.execute_input":"2024-05-02T01:40:12.744354Z","iopub.status.idle":"2024-05-02T01:40:12.750884Z","shell.execute_reply.started":"2024-05-02T01:40:12.744327Z","shell.execute_reply":"2024-05-02T01:40:12.749898Z"},"trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Creating Custom Dataset class for Toxic comments and Labels\nclass ToxicDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length, eval_mode: bool = False):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.eval_mode = eval_mode\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['comment_text']        \n        if not self.eval_mode:\n            label = self.data.iloc[idx]['label']\n        else:\n            label = 0\n\n        # Tokenize and encode the text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:14.952385Z","iopub.execute_input":"2024-05-02T01:40:14.952740Z","iopub.status.idle":"2024-05-02T01:40:14.961343Z","shell.execute_reply.started":"2024-05-02T01:40:14.952709Z","shell.execute_reply":"2024-05-02T01:40:14.960297Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Initializing tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = DistilBERT_Model(num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:17.830819Z","iopub.execute_input":"2024-05-02T01:40:17.831691Z","iopub.status.idle":"2024-05-02T01:40:18.203403Z","shell.execute_reply.started":"2024-05-02T01:40:17.831654Z","shell.execute_reply":"2024-05-02T01:40:18.202582Z"},"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Defining datasets and data loaders for train, validation, and test\ntrain_dataset = ToxicDataset(train_df, tokenizer, MAX_LEN)\nval_dataset = ToxicDataset(val_df, tokenizer, MAX_LEN)\ntest_output_set = ToxicDataset(cleaned_test_data, tokenizer, MAX_LEN, eval_mode=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False)\ntest_output_loader = DataLoader(test_output_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:20.881298Z","iopub.execute_input":"2024-05-02T01:40:20.882106Z","iopub.status.idle":"2024-05-02T01:40:20.890686Z","shell.execute_reply.started":"2024-05-02T01:40:20.882073Z","shell.execute_reply":"2024-05-02T01:40:20.889756Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n    # Validation evaluation after each epoch\n    model.eval()\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            _, predicted = torch.max(outputs, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_accuracy = correct_val / total_val\n    print(f'Epoch {epoch+1}/{EPOCHS}, Validation Accuracy: {val_accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:40:23.808992Z","iopub.execute_input":"2024-05-02T01:40:23.809346Z","iopub.status.idle":"2024-05-02T01:55:54.970425Z","shell.execute_reply.started":"2024-05-02T01:40:23.809316Z","shell.execute_reply":"2024-05-02T01:55:54.969395Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/3, Validation Accuracy: 0.9298\nEpoch 2/3, Validation Accuracy: 0.9215\nEpoch 3/3, Validation Accuracy: 0.9205\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Saving model\ntorch.save(model,\"dsbert_toxic_balanced.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:56:12.295828Z","iopub.execute_input":"2024-05-02T01:56:12.296479Z","iopub.status.idle":"2024-05-02T01:56:12.663001Z","shell.execute_reply.started":"2024-05-02T01:56:12.296447Z","shell.execute_reply":"2024-05-02T01:56:12.661930Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Evaluation on test data\nmodel.eval()\ntest_predictions = []\n\nwith torch.no_grad():\n    for batch in test_output_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs, 1)\n        test_predictions.extend(predicted.cpu().detach().numpy())\n\n# Convert predictions to DataFrame with 'ID' column\ntest_ids = cleaned_test_data['id']\npredictions_df = pd.DataFrame({'ID': test_ids, 'Label': test_predictions})\n\n# Save predictions to CSV\npredictions_df.to_csv('distilbert_nn2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:56:15.380497Z","iopub.execute_input":"2024-05-02T01:56:15.381154Z","iopub.status.idle":"2024-05-02T01:56:27.711298Z","shell.execute_reply.started":"2024-05-02T01:56:15.381120Z","shell.execute_reply":"2024-05-02T01:56:27.710304Z"},"trusted":true},"outputs":[],"execution_count":50}]}